{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e571f90a",
   "metadata": {},
   "source": [
    "# 1. Import libraries and loading config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1df20c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, lit, concat_ws, col\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "#loading config\n",
    "with open(os.getcwd()+\"/config.json\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d440e",
   "metadata": {},
   "source": [
    "# 2. Creating Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115fa900",
   "metadata": {},
   "source": [
    "## 2.1. Creating stations files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f0a81",
   "metadata": {},
   "source": [
    "### 2.1.1. Defining schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38a69455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated using copilot\n",
    "stationSchemaDmi = StructType([\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"features\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"geometry\", StructType([\n",
    "                StructField(\"coordinates\", ArrayType(DoubleType()), True),\n",
    "                StructField(\"type\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\"properties\", StructType([\n",
    "                StructField(\"anemometerHeight\", StringType(), True),\n",
    "                StructField(\"barometerHeight\", StringType(), True),\n",
    "                StructField(\"country\", StringType(), True),\n",
    "                StructField(\"created\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"operationFrom\", StringType(), True),\n",
    "                StructField(\"operationTo\", StringType(), True),\n",
    "                StructField(\"owner\", StringType(), True),\n",
    "                StructField(\"parameterId\", ArrayType(StringType()), True),\n",
    "                StructField(\"regionId\", StringType(), True),\n",
    "                StructField(\"stationHeight\", DoubleType(), True),\n",
    "                StructField(\"stationId\", StringType(), True),\n",
    "                StructField(\"status\", StringType(), True),\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"updated\", StringType(), True),\n",
    "                StructField(\"validFrom\", StringType(), True),\n",
    "                StructField(\"validTo\", StringType(), True),\n",
    "                StructField(\"wmoCountryCode\", StringType(), True),\n",
    "                StructField(\"wmoStationId\", StringType(), True)\n",
    "            ]), True)\n",
    "        ])\n",
    "    ), True),\n",
    "    StructField(\"timeStamp\", StringType(), True),\n",
    "    StructField(\"numberReturned\", IntegerType(), True),\n",
    "    StructField(\"links\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"href\", StringType(), True),\n",
    "            StructField(\"rel\", StringType(), True),\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\"title\", StringType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "#generated using copilot\n",
    "stationSchemaFrost = StructType([\n",
    "    StructField(\"@context\", StringType(), True),\n",
    "    StructField(\"@type\", StringType(), True),\n",
    "    StructField(\"apiVersion\", StringType(), True),\n",
    "    StructField(\"license\", StringType(), True),\n",
    "    StructField(\"createdAt\", StringType(), True),\n",
    "    StructField(\"queryTime\", DoubleType(), True),\n",
    "    StructField(\"currentItemCount\", IntegerType(), True),\n",
    "    StructField(\"itemsPerPage\", IntegerType(), True),\n",
    "    StructField(\"offset\", IntegerType(), True),\n",
    "    StructField(\"totalItemCount\", IntegerType(), True),\n",
    "    StructField(\"currentLink\", StringType(), True),\n",
    "    StructField(\"data\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"@type\", StringType(), True),\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"shortName\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True),\n",
    "            StructField(\"countryCode\", StringType(), True),\n",
    "            StructField(\"geometry\", StructType([\n",
    "                StructField(\"@type\", StringType(), True),\n",
    "                StructField(\"coordinates\", ArrayType(DoubleType()), True),\n",
    "                StructField(\"nearest\", BooleanType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"masl\", DoubleType(), True),\n",
    "            StructField(\"validFrom\", StringType(), True),\n",
    "            StructField(\"county\", StringType(), True),\n",
    "            StructField(\"countyId\", IntegerType(), True),\n",
    "            StructField(\"municipality\", StringType(), True),\n",
    "            StructField(\"municipalityId\", IntegerType(), True),\n",
    "            StructField(\"ontologyId\", IntegerType(), True),\n",
    "            StructField(\"stationHolders\", ArrayType(StringType()), True),\n",
    "            StructField(\"externalIds\", ArrayType(StringType()), True),\n",
    "            StructField(\"wigosId\", StringType(), True),\n",
    "            StructField(\"wmoId\", IntegerType(), True),\n",
    "            StructField(\"shipCodes\", ArrayType(StringType()), True),\n",
    "            StructField(\"icaoCodes\", ArrayType(StringType()), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c7f00",
   "metadata": {},
   "source": [
    "### 2.1.2. Data cleaning and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29fac0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/03 13:06:02 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /Users/nicklas.stiborg/Documents/medallion_architecture_project/bronze/dmi/dmiStations*.\n",
      "java.io.FileNotFoundException: File /Users/nicklas.stiborg/Documents/medallion_architecture_project/bronze/dmi/dmiStations* does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor146.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/07/03 13:06:02 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /Users/nicklas.stiborg/Documents/medallion_architecture_project/bronze/frost/frostStations*.\n",
      "java.io.FileNotFoundException: File /Users/nicklas.stiborg/Documents/medallion_architecture_project/bronze/frost/frostStations* does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor146.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "# paths for station files extracted from config file\n",
    "curr_path = os.getcwd()\n",
    "path_dmi = curr_path+config['import']['dmiExportPath']+'/dmiStations*'\n",
    "path_frost_met = curr_path+config['import']['frostExportPath']+'/frostStations*'\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"weatherSilverLayerApp\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# construction dmi_df\n",
    "dmi_station_df = spark.read.option(\"multiLine\", True).schema(stationSchemaDmi).json(path_dmi) # read all json file from path\n",
    "dmi_station_df_features = dmi_station_df.select(explode(\"features\").alias(\"feature\"))\n",
    "# flattening structure, please inspect json files for detailed levels\n",
    "dmi_df = dmi_station_df_features.select(\"feature.id\", \n",
    "                                     \"feature.properties.name\",\n",
    "                                     \"feature.properties.country\"\n",
    ")\n",
    "dmi_df = dmi_df.withColumn(\"source\", lit('DMI')) # adding source column\n",
    "# converting so that country codes matches frost data\n",
    "dmi_df = dmi_df.withColumn('country',\n",
    "              F.when(dmi_df.country == \"DNK\", \"DK\")\n",
    "              .when(dmi_df.country == \"GRL\", \"GL\")\n",
    "              .when(dmi_df.country == \"FRO\", \"FO\"))\n",
    "\n",
    "# applying uppercase to name column\n",
    "\n",
    "dmi_df = dmi_df.withColumn('name',\n",
    "              F.upper(dmi_df.name))\n",
    "\n",
    "#construction frost_df\n",
    "frost_station_df = spark.read.option(\"multiLine\", True).schema(stationSchemaFrost).json(path_frost_met) # read all json file from path\n",
    "frost_station_df_features = frost_station_df.select(explode(\"data\").alias(\"station\"))\n",
    "# flattening structure, please inspect json files for detailed levels\n",
    "frost_df = frost_station_df_features.select(\"station.id\",\n",
    "                                        \"station.name\",\n",
    "                                        \"station.countryCode\",\n",
    ")\n",
    "frost_df = frost_df.withColumn(\"source\", lit('FROST_MET')) # adding source column\n",
    "frost_df = frost_df.withColumnRenamed(\"countryCode\", \"country\")\n",
    "\n",
    "# applying uppercase to name column\n",
    "\n",
    "frost_df = frost_df.withColumn('name',\n",
    "              F.upper(frost_df.name))\n",
    "\n",
    "# appending and dropping duplicates\n",
    "dmi_df = dmi_df.drop_duplicates([\"id\"])\n",
    "frost_df = frost_df.drop_duplicates([\"id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee445947",
   "metadata": {},
   "source": [
    "### 2.1.3. Exporting to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bb5ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmi_export_location = os.getcwd() + config['export']['dmiDeltaPathStation']\n",
    "frost_export_location = os.getcwd() + config['export']['frostDeltaPathStation']\n",
    "dmi_df.write.format('parquet').save(dmi_export_location + str(datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")), mode=\"overwrite\")\n",
    "frost_df.write.format('parquet').save(frost_export_location + str(datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3874e95",
   "metadata": {},
   "source": [
    "## 2.2. Creating readings files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e9bf8",
   "metadata": {},
   "source": [
    "### 2.2.1. Defining schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb9d2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated using copilot\n",
    "readingsDmiSchema = StructType([\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"features\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"geometry\", StructType([\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"coordinates\", ArrayType(DoubleType()), True)\n",
    "            ]), True),\n",
    "            StructField(\"properties\", StructType([\n",
    "                StructField(\"created\", StringType(), True),  # or TimestampType() if parsed\n",
    "                StructField(\"observed\", StringType(), True),  # or TimestampType() if parsed\n",
    "                StructField(\"parameterId\", StringType(), True),\n",
    "                StructField(\"stationId\", StringType(), True),\n",
    "                StructField(\"value\", DoubleType(), True)\n",
    "            ]), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "\n",
    "# generated using copilot\n",
    "readingsFrostSchema = StructType([\n",
    "    StructField(\"@context\", StringType(), True),\n",
    "    StructField(\"@type\", StringType(), True),\n",
    "    StructField(\"apiVersion\", StringType(), True),\n",
    "    StructField(\"license\", StringType(), True),\n",
    "    StructField(\"createdAt\", StringType(), True),\n",
    "    StructField(\"queryTime\", DoubleType(), True),\n",
    "    StructField(\"currentItemCount\", IntegerType(), True),\n",
    "    StructField(\"itemsPerPage\", IntegerType(), True),\n",
    "    StructField(\"offset\", IntegerType(), True),\n",
    "    StructField(\"totalItemCount\", IntegerType(), True),\n",
    "    StructField(\"currentLink\", StringType(), True),\n",
    "    StructField(\"data\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"sourceId\", StringType(), True),\n",
    "            StructField(\"referenceTime\", StringType(), True),\n",
    "            StructField(\"observations\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"elementId\", StringType(), True),\n",
    "                    StructField(\"value\", DoubleType(), True),\n",
    "                    StructField(\"unit\", StringType(), True),\n",
    "                    StructField(\"level\", StructType([\n",
    "                        StructField(\"levelType\", StringType(), True),\n",
    "                        StructField(\"unit\", StringType(), True),\n",
    "                        StructField(\"value\", DoubleType(), True)\n",
    "                    ]), True),\n",
    "                    StructField(\"timeOffset\", StringType(), True),\n",
    "                    StructField(\"timeResolution\", StringType(), True),\n",
    "                    StructField(\"timeSeriesId\", IntegerType(), True),\n",
    "                    StructField(\"performanceCategory\", StringType(), True),\n",
    "                    StructField(\"exposureCategory\", StringType(), True),\n",
    "                    StructField(\"qualityCode\", IntegerType(), True)\n",
    "                ])\n",
    "            ), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e8c2b",
   "metadata": {},
   "source": [
    "### 2.2.2. Data cleaning and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30789919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----+---------+--------------------+\n",
      "|stationId|            observed|value|   source|            sourceId|\n",
      "+---------+--------------------+-----+---------+--------------------+\n",
      "| SN1135:0|2025-06-29T00:00:...| 14.2|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T00:10:...| 14.2|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T00:20:...| 14.2|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T00:30:...| 14.1|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T00:40:...| 14.0|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T00:50:...| 14.0|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T01:00:...| 13.8|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T01:10:...| 13.7|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T01:20:...| 13.6|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T01:30:...| 13.6|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T01:40:...| 13.5|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T01:50:...| 13.4|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T02:00:...| 13.4|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T02:10:...| 13.3|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T02:20:...| 13.3|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T02:30:...| 13.4|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T02:40:...| 13.4|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T02:50:...| 13.3|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T03:00:...| 13.2|FROST_MET|SN1135:0_2025-06-...|\n",
      "| SN1135:0|2025-06-29T03:10:...| 13.2|FROST_MET|SN1135:0_2025-06-...|\n",
      "+---------+--------------------+-----+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "+--------------------+--------------------+---------+-----+------+\n",
      "|                  id|            observed|stationId|value|source|\n",
      "+--------------------+--------------------+---------+-----+------+\n",
      "|2567630e-ea72-1b7...|2025-06-30T09:10:00Z|    06181| 17.9|   DMI|\n",
      "|2b4281f5-ff29-4d0...|2025-06-30T09:10:00Z|    06118| 18.0|   DMI|\n",
      "|3694ce54-b49b-b0d...|2025-06-30T09:10:00Z|    06132| 17.5|   DMI|\n",
      "|3965f646-2325-672...|2025-06-30T09:10:00Z|    06120| 17.7|   DMI|\n",
      "|3b6d1b29-7a81-343...|2025-06-30T09:10:00Z|    06074| 17.2|   DMI|\n",
      "|42813cc7-f16e-950...|2025-06-30T09:10:00Z|    06060| 17.4|   DMI|\n",
      "|441e20f8-f3ac-9f7...|2025-06-30T09:10:00Z|    06180| 18.5|   DMI|\n",
      "|5ca657b0-ae7d-10a...|2025-06-30T09:10:00Z|    04250|  7.8|   DMI|\n",
      "|62cd8d25-9cba-792...|2025-06-30T09:10:00Z|    06170| 17.2|   DMI|\n",
      "|936abddf-f651-0b1...|2025-06-30T09:10:00Z|    06070| 17.6|   DMI|\n",
      "|b07531da-76d3-202...|2025-06-30T09:10:00Z|    04220|  5.7|   DMI|\n",
      "|b789c7a3-714d-98a...|2025-06-30T09:10:00Z|    06104| 18.0|   DMI|\n",
      "|baf3cc17-d46b-795...|2025-06-30T09:10:00Z|    06080| 17.8|   DMI|\n",
      "|f539f502-daa2-e70...|2025-06-30T09:10:00Z|    06190| 17.1|   DMI|\n",
      "|f6bfd910-0cb6-963...|2025-06-30T09:10:00Z|    04231| 10.5|   DMI|\n",
      "|fa57e7f5-ddf9-ce3...|2025-06-30T09:10:00Z|    06110| 18.2|   DMI|\n",
      "|fb05a5a1-069e-3ce...|2025-06-30T09:10:00Z|    06030| 17.0|   DMI|\n",
      "|fc3f2c35-c241-93e...|2025-06-30T09:10:00Z|    06135| 17.5|   DMI|\n",
      "|02a6bc59-01e4-809...|2025-06-30T09:00:00Z|    06082| 17.4|   DMI|\n",
      "|03660889-2f66-bea...|2025-06-30T09:00:00Z|    06011| 11.7|   DMI|\n",
      "+--------------------+--------------------+---------+-----+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#paths for station files extracted from config file\n",
    "curr_path = os.getcwd()\n",
    "path_dmi = curr_path+config['import']['dmiExportPath']\n",
    "path_frost_met = curr_path+config['import']['frostExportPath']\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"weatherSilverLayerApp\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# list iterations to get all files in dir that matches dmiAirTemperature\n",
    "filesList = [\n",
    "    str(p) for p in Path(path_dmi).rglob(\"*.*\")\n",
    "    if p.parent != path_dmi and p.is_file() and \"dmiAirTemperature\" in p.name\n",
    "]\n",
    "\n",
    "dmi_weather_readings_df = spark.read.option(\"multiLine\", True).schema(readingsDmiSchema).json(filesList) # read all json file from path\n",
    "dmi_weather_readings_df_features = dmi_weather_readings_df.select(explode(\"features\").alias(\"feature\"))\n",
    "# flattening structure, please inspect json files for detailed levels\n",
    "dmi_weather_readings_df = dmi_weather_readings_df_features.select(\"feature.id\", \n",
    "                                     \"feature.properties.observed\",\n",
    "                                     \"feature.properties.stationId\",\n",
    "                                     \"feature.properties.value\"\n",
    ")\n",
    "dmi_weather_readings_df = dmi_weather_readings_df.withColumn(\"source\", lit('DMI')) # adding source column\n",
    "\n",
    "# list iterations to get all files in dir that matches dmiAirTemperature\n",
    "filesList = [\n",
    "    str(p) for p in Path(path_frost_met).rglob(\"*.*\")\n",
    "    if p.parent != path_frost_met and p.is_file() and \"frostMetAirTemperature\" in p.name\n",
    "]\n",
    "\n",
    "frost_weather_readings_df = spark.read.option(\"multiLine\", True).schema(readingsFrostSchema).json(filesList) # read all json file from path\n",
    "frost_weather_readings_df_features = frost_weather_readings_df.select(explode(\"data\").alias(\"data\"))\n",
    "# flattening structure, please inspect json files for detailed levels\n",
    "frost_weather_readings_df = frost_weather_readings_df_features.select(\"data.sourceId\", \n",
    "                                     \"data.referenceTime\",\n",
    "                                     \"data.observations.value\"\n",
    ")\n",
    "frost_weather_readings_df = frost_weather_readings_df.withColumn(\"source\", lit('FROST_MET')) # adding source column\n",
    "\n",
    "#renaming columns to match DMIs column names\n",
    "rename_map = {\n",
    "    \"sourceId\": \"stationId\",\n",
    "    \"referenceTime\": \"observed\",\n",
    "}\n",
    "for old_col, new_col in rename_map.items():\n",
    "    frost_weather_readings_df = frost_weather_readings_df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "#concating station id and timestamp to create a unique id, this will make the delta update easier when loading into the SQL\n",
    "frost_weather_readings_df = frost_weather_readings_df.withColumn(\"sourceId\", concat_ws(\"_\", frost_weather_readings_df.stationId, frost_weather_readings_df.observed))\n",
    "\n",
    "#converting type of value from list to float\n",
    "frost_weather_readings_df = frost_weather_readings_df.withColumn(\"value\", col(\"value\")[0].cast(\"float\"))\n",
    "\n",
    "frost_weather_readings_df.show()\n",
    "dmi_weather_readings_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f4a14",
   "metadata": {},
   "source": [
    "### 2.2.3. Exporting to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ad916de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/03 13:06:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/07/03 13:06:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/07/03 13:06:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/07/03 13:06:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/07/03 13:06:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "dmi_export_location = os.getcwd() + config['export']['dmiDeltaPathWr']\n",
    "frost_export_location = os.getcwd() + config['export']['frostDeltaPathWr']\n",
    "dmi_weather_readings_df.write.format('parquet').save(dmi_export_location + str(datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")), mode=\"overwrite\")\n",
    "frost_weather_readings_df.write.format('parquet').save(frost_export_location + str(datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")), mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
